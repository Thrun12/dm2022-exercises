{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utilities.load_data import load_iris_PC, load_iris, load_synthetic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Theory Questions\n",
    "1. Please provide a brief definition of the task of outlier detection: what is the goal, and what are the challenges. \n",
    "2. Please provide a definition of an outlier according to the following approaches and discuss their similarities and differences:\n",
    "    1. Statistical approaches \n",
    "    1. Distance-based (DB) \n",
    "    1. Local outlier factor (LOF) \n",
    "    1. Angle-based (ABOD) \n",
    "    1. Isolation-forest (IF) \n",
    "3. Create small examples, either visually or as a brief textual description, of an outlier that is NOT (well) captured by\n",
    "    1. DB-Outlier \n",
    "    1. LOF \n",
    "    1. ABOD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Practical Example\n",
    "Please consider the example data set below. Assuming Manhattan distance, please consider the following outlier detection setups.\n",
    "\n",
    "<img src=\"graphics/W5.02.png\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_data():\n",
    "    return np.array([\n",
    "        [1, 5], [2, 7], [3, 6], [3,7], [3,8], [4,5], [4,6], [4,7], [8,1], [8,4], [9,3], [9, 5]\n",
    "    ])\n",
    "X = grid_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For which parameters are the two highlighted objects outliers according to the distancebased definition (DB)?\n",
    "2. For which parameters are the two highlighted objects in-liers according to DB-outlier? \n",
    "3. In statistical approaches, a way to identify outliers in a given attribute is to single out all objects that deviate by more than three times the standard deviation from the mean in this attribute. Look at 1d and 2d plots to identify outliers.\n",
    "4. Determine the LOF for the two highlighted objects $p_1=(1,5)$, $p_2=(8,1)$ for $k=3$, $MinPts=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Comparing Isolation Forest and LOF with statistical methods\n",
    "For this exercise, we will be using sklearn to do outlier detection. \n",
    "We will use [LOF](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html) \n",
    "and [IF](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest).\n",
    "\n",
    "We will use the following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "X_grid = grid_data()\n",
    "X_iris, *_ = load_iris_PC()\n",
    "X_hics, *_ = load_synthetic_data()\n",
    "ax[0].scatter(*(X_grid.T))\n",
    "ax[0].set_title('Grid data')\n",
    "ax[1].scatter(*(X_iris.T))\n",
    "ax[1].set_title('Iris 2PC data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to LOF and IF, we will implement a simple version of `statistical_outliers` and `MDist`. \n",
    "For the former, we choose outliers as points where any of their features have a distance to the mean which exceeds $k$ standard deviations. \n",
    "For the latter, we normalize the data to have standard deviation 1 and then compute the $MDist(x, \\mu) = (x-\\mu)^T \\Sigma^{-1} (x-\\mu)$ where $\\Sigma$ is an estimate of the covariance matrix given by `np.cov`.\n",
    "`MDist` is going to take a threshold $k$, which decides the threshold for when a point is an outlier.\n",
    "\n",
    "Fill in the following two method stubs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_outliers(X, k=3):\n",
    "    # TODO implement statistical outliers.\n",
    "    # return vector y where y[i] is -1 if point i is an outlier and 0 otherwise.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDist(X, k=2):\n",
    "    # TODO implement KDist\n",
    "    # return vector y where y[i] is -1 if point i is an outlier and 0 otherwise.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code that runs the four algorithms on the different datasets.\n",
    "_Before_ running the code, think about which points can be choosen as outliers for the different methods and which points cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the resulting plots correspond to your expectations? If not, try to justify why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "k =  2\n",
    "fig, ax = plt.subplots(3,4, figsize=(16, 12))\n",
    "\n",
    "for i, data in enumerate(['Grid', 'Iris']):\n",
    "    n_neighbors  = 10\n",
    "    if data == 'Grid':\n",
    "        X           = grid_data()\n",
    "        n_neighbors = 2\n",
    "    elif data == 'Iris':\n",
    "        X, _        = load_iris_PC()\n",
    "\n",
    "    # Find outliers\n",
    "    y1 = statistical_outliers(X, k)\n",
    "    y2 = MDist(X, k)\n",
    "    y3 = IsolationForest(n_estimators=10).fit_predict(X)\n",
    "    y4 = LocalOutlierFactor(n_neighbors=n_neighbors).fit_predict(X)\n",
    "\n",
    "    # Plot data\n",
    "    ax[i, 0].set_title(\"Stats. box\")\n",
    "    cf = ax[i, 0].scatter(*(X.T), c=y1)\n",
    "    ax[i, 1].set_title(\"MDist\")\n",
    "    ax[i, 1].scatter(*(X.T), cmap=cf.get_cmap(), c=y2)\n",
    "    ax[i, 2].set_title(\"Isolation Forest\")\n",
    "    ax[i, 2].scatter(*(X.T), c=y3)\n",
    "    ax[i, 3].set_title(\"LOF\")\n",
    "    ax[i, 3].scatter(*(X.T), c=y4)\n",
    "    \n",
    "    ax[i, 0].set_ylabel(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: ABOD\n",
    "In this exercise, we will implement the exact ABOD scores as well as a simple sampling stratrgy for approximating the ABOD score.\n",
    "Recall, that the ABOD score is defined as\n",
    "\n",
    "$$\n",
    "ABOD(p) = \\text{var}_{x,y\\in DB} \\left[ \\frac{\\langle px, py \\rangle}{||px||_2||py||_2} \\right]\n",
    "$$\n",
    "\n",
    "where $px$ and $py$ are vectors from $p$ to $x$ and $y$, respectively.\n",
    "\n",
    "Below is a template for computing the ABOD values. We will not do the refinement step of the approximation algorithm.\n",
    "We will only compare full estimation of ABOD values with approximations.\n",
    "Implement the TODO and run the experiment below.\n",
    "\n",
    "Please answer the following questions:\n",
    "1. What can we tell from the experiment in terms of `sample_size` and quality of approximations?\n",
    "2. If you were to implement the refinement step, how many samples would you use for selecting candidates?\n",
    "3. Why does sampling become slower than computing true ABOD when number of samples increase?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def abod(i, X):\n",
    "    \"\"\"\n",
    "        Compute ABOD for point x_i by comparing x_i to all other points in X.\n",
    "        return score\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    p = X[i]\n",
    "    X_   = np.delete(X, i, 0) # Remove x_i from X\n",
    "    \n",
    "    # TODO compute ABOD\n",
    "    abod = 0\n",
    "    \n",
    "    return abod \n",
    "\n",
    "def fast_abod(i, X, sample_size=20):\n",
    "    \"\"\"\n",
    "        Draw a random sample of `sample_size` points (not including x_i) and run the full `abod` function above on the subsample.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    probs = np.ones((n,)) * (1/(n-1))                                         # Equal probability for all points but x_i\n",
    "    probs[i] = 0.\n",
    "    X_ = X[np.random.choice(X.shape[0], sample_size, replace=False, p=probs)] # Choose `sample_size` points from X at random.\n",
    "    X_ = np.concatenate([X[i:i+1], X_], axis=0)                               # Add x_i to X_ at index 0 and call abod with index 0\n",
    "    return abod(0, X_)\n",
    "\n",
    "def ABOD(X, sample_size=-1):\n",
    "    \"\"\"\n",
    "        Compute the ABOD value for all points in X \n",
    "    \"\"\"\n",
    "    fn = lambda i, X: fast_abod(i, X, sample_size) \n",
    "    if sample_size == -1: fn = abod\n",
    "     \n",
    "    A = []\n",
    "    for i in range(X.shape[0]):\n",
    "        A.append(fn(i, X))\n",
    "    A = np.array(A)\n",
    "    return np.clip(A, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_synthetic_data()\n",
    "sel = np.argsort(y)[::-1] # Choose point such that we have some outliers to play with.\n",
    "X = X[sel[:300]]\n",
    "y = y[sel[:300]]\n",
    "\n",
    "# Choose the first 300 rows and only the first two dimensions. \n",
    "# We can't choose too many rows as we are looking at an O(n^3) algorithm here.\n",
    "X = X[:,8:10]\n",
    "n, d = X.shape\n",
    "\n",
    "sample_sizes = [2, 5, 10, 20, 40, 80, 160, n-1]\n",
    "fig, ax = plt.subplots(len(sample_sizes), 2, figsize=(12, len(sample_sizes)*4))\n",
    "\n",
    "times = []\n",
    "runs  = 10\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(runs): A1 = ABOD(X)\n",
    "t_slow = time.time() - t0\n",
    "t_slow /= runs\n",
    "\n",
    "for i, ss in enumerate(sample_sizes):\n",
    "    t0 = time.time()\n",
    "    for _ in range(runs): A2 = ABOD(X, ss)\n",
    "    t_fast = time.time() - t0\n",
    "    t_fast /= runs\n",
    "    \n",
    "    times.append((t_slow, t_fast))\n",
    "    cf = ax[i,0].scatter(*(X.T), c=A1)\n",
    "    ax[i, 0].set_title(\"Slow\")\n",
    "    fig.colorbar(cf, ax=ax[i, 0])\n",
    "    \n",
    "    cf = ax[i,1].scatter(*(X.T), c=A2)\n",
    "    ax[i,1].set_title(\"Fast, ss=%i\" % ss)\n",
    "    fig.colorbar(cf, ax=ax[i, 1])    \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "times = np.array(times).T\n",
    "print(times.shape)\n",
    "print(len(sample_sizes))\n",
    "ax.plot(sample_sizes, times[0], 'r-', label=\"Slow\")\n",
    "ax.plot(sample_sizes, times[1], 'b',  label=\"Approx.\")\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
