{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sticky-texas",
   "metadata": {},
   "source": [
    "# Data Mining - Handin 1 - Clustering \n",
    "Welcome to the handin on clustering algorithms and outlier detection. \n",
    "This handin corresponds to the topics in Week 5--9 in the course.\n",
    "\n",
    "The handin IS \n",
    "* done individually\n",
    "* worth 10% of the final grade\n",
    "\n",
    "For the handin, you will prepare a report in PDF format, by exporting the Jupyter notebook. \n",
    "Please submit\n",
    "1. The jupyter notebook file with your answers\n",
    "2. The PDF obtained by exporting the jupyter notebook\n",
    "\n",
    "Submit both files on Brightspace no later than **March 8th kl. 8.59**.\n",
    "\n",
    "**The grading system**: Tasks are assigned a number of points based on the difficulty and time to solve it. The sum of\n",
    "the number of points is 100. For the maximum grade you need to get at least _80 points_. The minimum grade (02 in the Danish scale)\n",
    "requires **at least** 30 points, with at least 8 points on of the first three Parts (Part 1,2,3) and 6 points in the last part (Part 4).\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!conda install --yes --prefix {sys.prefix} seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT TOUCH\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_SEED = 132414\n",
    "## DO NOT TOUCH\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "poke = pd.read_csv(\"./data/Pokemon.csv\")\n",
    "toy = poke[poke['Type_1'].isin(['Fire', 'Ice'])].sample(n=12, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-helmet",
   "metadata": {},
   "source": [
    "# Part 1 Theoretical Questions\n",
    "\n",
    "## Task 1.1 K-Means and DBScan\n",
    "\n",
    "### Task 1.1.1 (2 points)\n",
    "Give the data set below:\n",
    "Compute cluster assignments using k-means and k = 2, with initial centroids being (-1, -1) and (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {'Ice':'Blue', 'Fire':'Red'}\n",
    "X_kmeans = toy[[\"Attack\", \"Defense\"]]\n",
    "\n",
    "scaler = StandardScaler().fit(X_kmeans)\n",
    "X_scaled = scaler.transform(X_kmeans)\n",
    "\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.8, c=toy['Type_1'].map(color_map))\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-green",
   "metadata": {},
   "source": [
    "### Task 1.1.2 (1 point)\n",
    "Show two examples with two different initial cluster assignments that lead to a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-report",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-market",
   "metadata": {},
   "source": [
    "### Task 1.1.3 (4 points)\n",
    "Compute the dendrograms using single-link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-judgment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "affecting-issue",
   "metadata": {},
   "source": [
    "### Task 1.1.4 (2 points)\n",
    "Run density-based clustering with $\\epsilon=1$ and $MinPts=2$. What clusters do you obtain? **For this exercise you can use the DBSCAN from sklearn to check your results** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-fundamental",
   "metadata": {},
   "source": [
    "## Task 1.2 Elliptic data set (4 points)\n",
    "After looking at the dataset below, you realize that the clusters are elliptic rather than spherical. You want to detect the red outlier point, assuming you know that is an outlier. \n",
    "\n",
    "Which approach would be the most obvious to find the red outlier? Please check the box and motivate your answer below:\n",
    "- [ ] Distance based approach\n",
    "- [ ] Angle based approach\n",
    "- [ ] Depth based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-lobby",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_new = np.array([[1.42, 1.131], # Red \n",
    "                [1.34, 0.3],\n",
    "                [1.44, 0.30],\n",
    "                [1.5, 0.5],\n",
    "                [1.48, 0.65],\n",
    "                [1.37, 0.66],\n",
    "                [1.3, 0.50],\n",
    "                 ])\n",
    "\n",
    "plt.scatter(D_new[:, 0], D_new[:, 1], alpha=0.8, c = ['red' if i == 0 else 'blue' for i in range(len(D_new))])\n",
    "plt.axis([1, 2, 0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-corporation",
   "metadata": {},
   "source": [
    "## Task 1.3 Theoretical questions (4 points)\n",
    "1. You are given a measure $d(x,y) = |x-y|$, show that the measure is a metric \n",
    "2. Show that $\\hat{\\Sigma}=\n",
    "\\frac{1}{n}\\sum_{i=1}^n (x_i -\\hat{\\mu}^\\top)\\cdot(x_i -\\hat{\\mu}^\\top)^\\top=E[(X-\\hat{\\mu})(X-\\hat{\\mu})^\\top]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-portable",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-situation",
   "metadata": {},
   "source": [
    "# Part 2 Exploratory data analysis\n",
    "In this section, you will perform preliminary analyses on your data. These preliminary analysis are useful to understand how the data behaves, before running complex algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = poke[poke['Type_1'].isin(['Fire', 'Ice'])]\n",
    "data_np = toy.drop(columns = ['#', 'Name', 'Type_1', 'Type_2', 'Generation', 'Legendary']).to_numpy()\n",
    "headers = [\"Name\", \"Type_1\", \"Type_2\", \"Total\", \"HP\", \"Attack\", \"Defense\", \"Sp. Atk\", \"Sp. Def\", \"Speed\", \"Generation\", \"Legendary\"]\n",
    "X = data_np[:,:5]\n",
    "y = data_np[:,6]\n",
    "y = y.astype(int) - 1\n",
    "rows, cols = np.shape(X)\n",
    "toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-world",
   "metadata": {},
   "source": [
    "## Task 2.1 Correlation matrix\n",
    "### Task 2.1.1 (6 points)\n",
    "Compute the correlation matrix (not covariance matrix) among all the attributes in the code box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_np\n",
    "print(\"your code here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-consumption",
   "metadata": {},
   "source": [
    "### Task 2.1.2 (2 points)\n",
    "We can also plot the correlation matrix as seen below. What is the relationship between the correlation matrix and the covariance matrix? Please check the correct box below. \n",
    "\n",
    "- [ ] The correlation matrix contains the unnormalized covariance values\n",
    "- [ ] The correlation matrix contains the normalized covariance values\n",
    "- [ ] The covariance matrix contains the variance of the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(poke.corr(),annot=True,linewidths=.5, cmap=\"YlGnBu\", annot_kws={\"fontsize\":8}, vmax=1)\n",
    "plt.title('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-democrat",
   "metadata": {},
   "source": [
    "### Task 2.1.3 (6 points)\n",
    "In this task, we reason about the covariance matrices. Normalize the features using standard score normalization and range normalization. Plot the **covariance** matrices for\n",
    "1. The unnormalized data\n",
    "2. The [standard score normalized features](https://en.wikipedia.org/wiki/Standard_score)\n",
    "3. The range (min-max) normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_np\n",
    "\n",
    "print(\"Your code here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-participation",
   "metadata": {},
   "source": [
    "### Task 2.1.4 (6 points)\n",
    "Note how the covariance matrix changes with different normalization schemes and reason on why such behaviour appears.\n",
    "You should notice some differences. Why are these differences appearing?\n",
    "\n",
    "\n",
    "- [ ] Range normalization preserves the variance. Therefore, features are directly comparable.\n",
    "- [ ] Standard score normalization preserves the variance. Therefore, features are directly comparable.\n",
    "- [ ] Both methods normalize in such a way, that it makes sense to compare the different covariance values to each other.\n",
    "- [ ] None of the methods normalize in such a way that it makes sense to compare the different covariance values to each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-optimization",
   "metadata": {},
   "source": [
    "## Task 2.2 Normal distribution\n",
    "\n",
    "### Task 2.2.1 (6 points)\n",
    "Sometimes it is convenient to know whether a variable is close to a normal distribution. Implement a method norm_dist that: \n",
    "    1) Inputs the number of buckets $b$ and a vector $x$ of values\n",
    "    2) Outputs the sum of the absolute differences of the buckets between the a histogram with $b$ buckets of a normal variable with $\\mu,\\sigma$ deviation corresponding sample mean and standard deviation of the input vector and the histogram of the input vectors with $b$ buckets. The sum of the differences is computed as \n",
    "    $$\\sum_{i=1}^b |H_X(i) - H_{\\mathcal{N}}(i)|$$ \n",
    "\n",
    "Where $H_X(i)$ is the i-th bucket of the histogram of $x$ and $H_\\mathcal{N}(i)$ is the i-th bucket of the hisotgram obtained from the normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$. \n",
    "\n",
    "Use the norm function from Scipy to get the normal distribution to subtract from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "## Our data comes from the variable X\n",
    "## print(X)\n",
    "X = data_np\n",
    "def norm_dist(x, b): \n",
    "    dist = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-province",
   "metadata": {},
   "source": [
    "### Task 2.2.2 (1 point)\n",
    "Is the method in Task 2.2.1 a good method? Is it robust to outliers? Run your code on each columns of the dataset.\n",
    "What is the column with the largest distance? Compare the norm_dist of each attribute feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data is still from X\n",
    "\n",
    "print(\"YOUR CODE HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-cotton",
   "metadata": {},
   "source": [
    "### Task 2.2.3 (2 points)\n",
    "Now look at the method below. This is called a Quantile-Quantile [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot). \n",
    "\n",
    "Discuss why this method is more robust than the one we proposed in Task 2.2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-camel",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-departure",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from matplotlib import gridspec\n",
    "\n",
    "plt.tight_layout()\n",
    "_, n = data.shape\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(8, 30))\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=(n-1), figure=fig)\n",
    "for i in np.arange(3,n): \n",
    "    x = data[headers[i]]\n",
    "    r = i-1\n",
    "    qq = fig.add_subplot(spec[r, 1]) \n",
    "    stats.probplot(x, plot=qq)\n",
    "    h = fig.add_subplot(spec[r, 0])\n",
    "    h.set_title(headers[i])\n",
    "    h.hist(x, bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-correspondence",
   "metadata": {},
   "source": [
    "# Part 3 Cluster Analysis\n",
    "In this section, you will perform cluster analysis of the dataset above and modify clustering algorithms to achieve better results. \n",
    "\n",
    "## Task 3.1\n",
    "\n",
    "### Task 3.1.1 (6 points)\n",
    "Implement and use the elbow method detect the number of clusters . For plotting you can use the **silhouette coefficient.**\n",
    "\n",
    "Use the \"Total\" and \"Stp. Atk\" features of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = toy[[\"Total\", \"Sp. Atk\"]].to_numpy()\n",
    "print(\"YOUR CODE HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-disabled",
   "metadata": {},
   "source": [
    "**YOUR TEXT HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-consultancy",
   "metadata": {},
   "source": [
    "### Task 3.1.2 (2 points)\n",
    "Run k-means on the dataset X, with the number of clusters detected in the previous exercise.\n",
    "\n",
    "**Note**: you can use the KMeans implementation from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "X = toy[[\"Total\", \"Sp. Atk\"]].to_numpy()\n",
    "\n",
    "# Necessary Data normalization!\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "\n",
    "clusters = []\n",
    "\n",
    "plt.scatter(X_norm[:, 0], X_norm[:, 1], alpha=0.8, c=clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-testing",
   "metadata": {},
   "source": [
    "### Task 3.1.3 (4 points)\n",
    "Implement Kernel K-means and the Gaussian Kernel. \n",
    "\n",
    "The Gaussian kernel is defined as in the following equation:\n",
    "\n",
    "$$\n",
    "K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\exp \\left(-\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|^{2}}{2 \\sigma^{2}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "X = toy[[\"Total\", \"Sp. Atk\"]].to_numpy()\n",
    "\n",
    "# Necessary Data normalization!\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=0.8): \n",
    "    k = 0 \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return k\n",
    "\n",
    "\n",
    "def kernel_kmeans(X, n_clusters, kernel=gaussian_kernel, iters=100, error=.01): \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return clusters\n",
    "\n",
    "\n",
    "clusters = kernel_kmeans(X_norm, NUMBER_OF_CLUSTERS)\n",
    "\n",
    "scaler = StandardScaler().fit(X_norm)\n",
    "X_scaled = scaler.transform(X_norm)\n",
    "clusters = kernel_kmeans(X_scaled, SOME_AMOUNT_OF_CLUSTERS)\n",
    "\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.8, c=clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-badge",
   "metadata": {},
   "source": [
    "## Task 3.2 Clustering quality\n",
    "\n",
    "### Task 3.2.1 (4 points)\n",
    "Now we will implement, *mutual information* and a measure for clustering quality.\n",
    "\n",
    "Entropy for a clustering is $H(C) = - \\sum_{i=1}^{k}{p_{C_i} \\log {p_{C_i}}}$.\n",
    "\n",
    "Mutual information for two clusterings $U$ and $V$ is defined as $\\text{MI}(U,V) = \\sum\\limits^{|U|}_{i=1} \\sum\\limits^{|V|}_{j=1} \\frac{|U_i \\cap V_j|}{N} \\log  N \\frac{|U_i \\cap V_j|}{|U_i||V_j|}$.\n",
    "We first need to implement entropy and then we will define mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(C):\n",
    "    # Let C be a list of clusters\n",
    "    entropy = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def MI(C1, C2):\n",
    "    mi = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return mi\n",
    "\n",
    "def NMI(C1, C2):\n",
    "    mi = MI(C1, C2)\n",
    "    h_c1 = entropy(C1)\n",
    "    h_c2 = entropy(C2)\n",
    "    return mi/(np.sqrt(h_c1 * h_c2))\n",
    "    \n",
    "\n",
    "X = toy[[\"Total\", \"Sp. Atk\"]].to_numpy()\n",
    "\n",
    "# Necessary Data normalization!\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-wisconsin",
   "metadata": {},
   "source": [
    "### Task 3.2.2 (4 points)\n",
    "Plot the NMI (Use the provided NMI function from the previous task) among the class labels $y$ and the clusters you found with k-means in Task 3.1. Make sure that the number of clusters and the number of class labels is the same (2).\n",
    "- Reason about the measure, is the measure influenced by the size of the clusters?  \n",
    "- What does the measure capture? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-hollywood",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###YOUR CODE HERE\n",
    "class_labels = np.array(toy[\"Type_1\"])\n",
    "class_labels = [1 if x == \"Fire\" else 0 for x in class_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-smooth",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3.2.3 (4 points)\n",
    "Provide an implementation of purity. Recall that purity is the weighted sum of the individual $purity_i = \\frac{1}{|C_i|} \\max_{j=1..k}\\{n_{ij}\\}$ values where $n_{ij}$ is the number of common points in cluster $C_i$\n",
    "and ground-truth clusters obtained from the labels $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "T = np.array([]) # Ground-truth clusters\n",
    "C = np.array([]) # Clusters obtained by k-means\n",
    "### YOUR CODE HERE\n",
    "\n",
    "## C is the clustering from k-means and T is the ground truth cluster assignments.\n",
    "def purity(C, T):\n",
    "    purity = 0\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return purity\n",
    "\n",
    "print('Purity: {}, NMI: {}'.format(purity(C,T), NMI(C,T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-routine",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3.2.4 (2 points)\n",
    "\n",
    "Plot purity and compare purity with NMI. Which measure is preferable?\n",
    "\n",
    "- [ ] NMI is preferable because it uses all the points\n",
    "- [ ] Purity is preferable because it is less computational demanding\n",
    "- [ ] NMI is preferrable because it does not favor small clusters\n",
    "- [ ] Purity is preferrable because it tends to favor balanced clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-craft",
   "metadata": {},
   "source": [
    "### Task 3.3 DBScan\n",
    "\n",
    "\n",
    "### Task 3.3.1 (1 point)\n",
    "\n",
    "Run DB-Scan with parameters $\\varepsilon=0.1, minPts=2$ from sklearn and compare the results with k-means. Which of the two achieve a better NMI? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER\n",
    "\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=2).fit(X_norm)\n",
    "\n",
    "clusters_dbs = dbscan.labels_\n",
    "plt.scatter(X_norm[:, 0], X_scaled[:, 1], alpha=0.8, c=clusters_dbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-binary",
   "metadata": {},
   "source": [
    "### Task 3.3.2 (4 points)\n",
    "DBScan requires tuning of parameters $\\varepsilon, MinPts$. Implement the heuristic strategy in the slides to find the best parameters. Run the code and see whether the results with DBScan improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_dbscan(X): \n",
    "    eps = 0\n",
    "    min_pts = (X.shape[1] *2)-1\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-significance",
   "metadata": {},
   "source": [
    "### Task 3.3.3 (6 points)\n",
    "Implement a simple subspace clustering algorithm.\n",
    "1. Take all subsets of 2,3 attributes. Beware that you should only use the numerical attributes.\n",
    "2. Run dbscan on each subset. \n",
    "3. Compute NMI for each subset. \n",
    "4. Keep the k subsets with the largest NMI. \n",
    "    \n",
    "**Note**: You may have to experiment a lot with eps and MinPts to get reasonable clusterings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Data normalization!\n",
    "X_pt = toy[[\"Total\", \"HP\", \"Attack\", \"Defense\", \"Sp. Atk\", \"Sp. Def\", \"Speed\", \"Generation\"]].to_numpy()\n",
    "X_norm_pt = (X_pt - X_pt.min(0)) / X_pt.ptp(0)\n",
    "print(\"YOUR CODE HERE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-structure",
   "metadata": {},
   "source": [
    "### Task 3.3.4 (4 points)\n",
    "Analyze the following from Task 3.3.3\n",
    "1. Advantages and disadvantages of the proposed algorithm\n",
    "2. Which subsets of attributes give the best results? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-defeat",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-dakota",
   "metadata": {},
   "source": [
    "# Part 4 Outlier detection\n",
    "In this exercise we will work with outlier detection techniques and analyze their performance on the small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = toy[[\"Attack\", \"Defense\"]].to_numpy()\n",
    "\n",
    "X_norm = (X_small - X_small.min(0)) / X_small.ptp(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-recorder",
   "metadata": {},
   "source": [
    "## Task 4.1 (DBoutliers)\n",
    "We will now compare two outlier detection techniques.\n",
    "### Task 4.1.1 (6 points)\n",
    "We will first implement a simple distance-based outlier detector. This is the distance-based outlier detection from the lectures, where a point is considered an outlier if at most a fraction pi of the other points have a distance less of than eps to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBOutliers(X, eps, pi): \n",
    "    outliers = None\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-traveler",
   "metadata": {},
   "source": [
    "## Task 4.1.2 (2 points)\n",
    "DBOutliers requires tuning the parameters eps, pi. Discuss how the results vary with those parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-distribution",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-holocaust",
   "metadata": {},
   "source": [
    "### Task 4.1.3 (3 points)\n",
    "**NOTE** This is hard but also fun. Since it is not impacting the grade too much, you can choose to invent something new.\n",
    "\n",
    "Propose a heuristic method to tune parameters eps, pi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_dboutliers(X): \n",
    "    eps = 0\n",
    "    pi = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return eps, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-vision",
   "metadata": {},
   "source": [
    "## Task 4.2 LOF (2 points)\n",
    "Using the parameters eps=0.2, pi=0.2 compare the results of DBOutliers with those of LOF implemented in Week 9. What outliers do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Your code here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-davis",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**YOUR ANSWER**\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
